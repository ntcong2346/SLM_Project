import streamlit as st
import requests
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- C·∫§U H√åNH TRANG CHUY√äN NGHI·ªÜP ---
st.set_page_config(
    page_title="Bio-SLM AI Assistant",
    page_icon="üß¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CSS T√ôY CH·ªàNH ƒê·ªÇ L√ÄM ƒê·∫∏P UI ---
st.markdown("""
    <style>
    .main { background-color: #0e1117; }
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; border: 1px solid #30363d; }
    .stSidebar { background-color: #161b22; border-right: 1px solid #30363d; }
    h1 { color: #58a6ff; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
    .status-box { padding: 10px; border-radius: 10px; border: 1px solid #30363d; background-color: #0d1117; margin-bottom: 10px; }
    </style>
    """, unsafe_allow_html=True)

# --- TI√äU ƒê·ªÄ V√Ä GI·ªöI THI·ªÜU ---
col1, col2 = st.columns([1, 4])

with col2:
    st.title("Bio-SLM AI Assistant")
    st.markdown("*H·ªá th·ªëng RAG h·ªó tr·ª£ h·ªçc t·∫≠p Sinh h·ªçc 12 d·ª±a tr√™n m√¥ h√¨nh ng√¥n ng·ªØ nh·ªè (SLM)*")

st.divider()

# --- H√ÄM KH·ªûI T·∫†O RAG ---
@st.cache_resource
def init_knowledge_base():
    data_path = "./data"
    if not os.path.exists(data_path) or not os.listdir(data_path):
        return None
    
    documents = []
    for filename in os.listdir(data_path):
        file_path = os.path.join(data_path, filename)
        if filename.endswith(".txt"):
            loader = TextLoader(file_path, encoding="utf-8")
            documents.extend(loader.load())
    
    if not documents: return None

    # Chia nh·ªè vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n tri th·ª©c chu·∫©n
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    chunks = text_splitter.split_documents(documents)
    
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma.from_documents(chunks, embeddings)
    return vector_db

# --- SIDEBAR: QU·∫¢N L√ù H·ªÜ TH·ªêNG ---
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh SLM")
    
    # Tr·∫°ng th√°i RAG
    with st.container():
        try:
            vector_db = init_knowledge_base()
            if vector_db:
                st.success("‚óè Kho ki·∫øn th·ª©c RAG: S·∫µn s√†ng")
            else:
                st.warning("‚óè Kho ki·∫øn th·ª©c RAG: Tr·ªëng (data/)")
        except Exception as e:
            st.error(f"‚óè L·ªói RAG: {e}")

    # Th√¥ng s·ªë hi·ªáu nƒÉng SLM
    st.markdown("---")
    st.subheader("Th√¥ng s·ªë SLM")
    st.write(f"**Model:** Llama-3.1-8B-Instant")
    st.write(f"**Ki·∫øn tr√∫c:** SLM (Small Language Model)")
    st.write(f"**Optimization:** Groq LPU Inference")
    
    # Ngu·ªìn tr√≠ch d·∫´n
    st.markdown("---")
    st.subheader("Ngu·ªìn ki·∫øn th·ª©c RAG")
    source_container = st.empty()

# --- KHUNG CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! T√¥i l√† tr·ª£ l√Ω SLM ƒë√£ ƒë∆∞·ª£c n·∫°p ki·∫øn th·ª©c Sinh h·ªçc 12. B·∫°n c·∫ßn t√¨m hi·ªÉu v·ªÅ ch·ªß ƒë·ªÅ n√†o?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("H·ªèi t√¥i v·ªÅ Di truy·ªÅn, Ti·∫øn h√≥a, Sinh th√°i..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("üîÑ SLM ƒëang tr√≠ch xu·∫•t tri th·ª©c..."):
            context = ""
            sources = []
            
            # 1. T√¨m ki·∫øm RAG t·ª´ file tri th·ª©c
            if vector_db:
                docs = vector_db.similarity_search(prompt, k=2)
                context = "\n\n".join([d.page_content for d in docs])
                sources = [d.page_content[:200] + "..." for d in docs]

            if sources:
                with source_container.container():
                    for i, s in enumerate(sources):
                        st.caption(f"ƒêo·∫°n tr√≠ch {i+1}:")
                        st.info(s)

            # 2. G·ªçi SLM qua Groq API
            try:
                api_key = st.secrets["GROQ_API_KEY"] 
                url = "https://api.groq.com/openai/v1/chat/completions"
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                # C·∫•u h√¨nh tham s·ªë chu·∫©n SLM
                data = {
                    "model": "llama-3.1-8b-instant", 
                    "messages": [
                        {
                            "role": "system", 
                            "content": f"B·∫°n l√† chuy√™n gia Sinh h·ªçc 12 d·∫°ng SLM. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn d·ª±a tr√™n tri th·ª©c: {context}"
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.4
                }
                
                response = requests.post(url, json=data, headers=headers)
                
                if response.status_code == 200:
                    res_text = response.json()['choices'][0]['message']['content']
                    st.markdown(res_text)
                    st.session_state.messages.append({"role": "assistant", "content": res_text})
                else:
                    st.error(f"L·ªói API: {response.status_code}")
                    
            except Exception as e:
                st.error(f"L·ªói k·∫øt n·ªëi SLM Cloud: {e}")